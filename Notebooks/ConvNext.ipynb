{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install --upgrade keras-cv\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install opencv-python\n",
    "!pip install numpy\n",
    "!pip install Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U keras-cv-attention-models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for data and model saving\n",
    "data_dir = '/content/drive/MyDrive/Orange_Dataset/train'\n",
    "test_dir = '/content/drive/MyDrive/Orange_Dataset/test'\n",
    "convnext_path = '/content/drive/MyDrive/model_convnext.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=30,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Data generator for testing (no augmentation, only rescaling)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Training and validation data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=128,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=128,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Testing data generator (no augmentation applied here)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=128,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ConvNeXt base model using timm\n",
    "# Use torch to load pretrained weights\n",
    "convnext_model = timm.create_model('convnext_tiny', pretrained=True)\n",
    "convnext_model.eval()\n",
    "\n",
    "# Freeze all the layers\n",
    "for param in convnext_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define a wrapper that makes it compatible with Keras (TensorFlow) via tf.keras.Model\n",
    "# We'll extract features from PyTorch, save them as numpy arrays, and feed them to Keras\n",
    "# So here we define just the classification head\n",
    "def build_convnext_head(num_classes=3):\n",
    "    input_tensor = Input(shape=(768,))  # 768 is the output dimension of convnext_tiny\n",
    "    x = Dense(256, activation='relu')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    output_tensor = Dense(num_classes, activation='softmax')(x)\n",
    "    return Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "convnext_head = build_convnext_head(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Remove the classification head of ConvNeXt to get features\n",
    "class ConvNeXtFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ConvNeXtFeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(*list(model.children())[:-2])  # Remove the final classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.nn.functional.adaptive_avg_pool2d(x, (1, 1))  # Global avg pool\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "feature_extractor = ConvNeXtFeatureExtractor(convnext_model).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define preprocessing for images as expected by ConvNeXt\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Extract features and labels from a generator\n",
    "def extract_features(generator, feature_extractor):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for i in tqdm(range(len(generator))):\n",
    "        x_batch, y_batch = generator[i]\n",
    "        for img in x_batch:\n",
    "            img_pil = Image.fromarray((img * 255).astype(np.uint8))  # Convert from float32 to uint8 PIL image\n",
    "            img_torch = preprocess(img_pil).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                feat = feature_extractor(img_torch).squeeze().numpy()\n",
    "            features.append(feat)\n",
    "        labels.append(y_batch)\n",
    "\n",
    "    features = np.array(features)\n",
    "    labels = np.vstack(labels)\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(train_generator, feature_extractor)\n",
    "val_features, val_labels = extract_features(val_generator, feature_extractor)\n",
    "test_features, test_labels = extract_features(test_generator, feature_extractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(input_dim, num_classes):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Input dimension depends on ConvNeXt output (usually 768 for tiny model)\n",
    "classifier = build_classifier(input_dim=train_features.shape[1], num_classes=3)\n",
    "classifier.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Define the classifier\n",
    "classifier = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(train_features.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # 3 classes: Canker, Melanose, Healthy\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "classifier.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the classifier\n",
    "history = classifier.fit(\n",
    "    train_features, train_labels,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained classifier model\n",
    "classifier.save(convnext_path)\n",
    "print(\"✅ Classifier model saved as 'convnext_classifier_head.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier on test set\n",
    "test_loss, test_accuracy = classifier.evaluate(test_features, test_labels)\n",
    "print(f\"\\n✅ Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Get the true and predicted class indices\n",
    "true_classes = np.argmax(test_labels, axis=1)\n",
    "pred_classes = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_classes, pred_classes)\n",
    "\n",
    "# Get class labels from the generator\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Plot using seaborn heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\" Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Step 1: Convert one-hot labels to class indices\n",
    "y_true = np.argmax(test_labels, axis=1)\n",
    "y_pred = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "# Step 2: Class names from generator\n",
    "class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Step 3: Classification report (precision, recall, F1)\n",
    "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(report_df)\n",
    "\n",
    "# Optional: Save report to CSV\n",
    "report_df.to_csv(\"classification_report_convnext.csv\")\n",
    "\n",
    "# Step 4: Print overall scores\n",
    "overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "macro_precision = precision_score(y_true, y_pred, average='macro')\n",
    "macro_recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"\\n Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "print(f\" Macro F1-score: {macro_f1:.4f}\")\n",
    "print(f\" Weighted F1-score: {weighted_f1:.4f}\")\n",
    "print(f\" Macro Precision: {macro_precision:.4f}\")\n",
    "print(f\" Macro Recall: {macro_recall:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
